# Configuration for DeepSeek-Coder 6.7B model
model:
  name: "deepseek-ai/deepseek-coder-6.7b-base"
  type: "causal_lm"
  use_flash_attention: true
  gradient_checkpointing: true
  trust_remote_code: true

# LoRA configuration
lora:
  r: 24
  alpha: 48
  dropout: 0.08
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "down_proj", "up_proj"]
  use_rslora: true
  
# Quantization settings
quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"

# Training hyperparameters  
training:
  output_dir: "training/models/deepseek-coder-6.7b-solidity-auditor"
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8
  num_train_epochs: 6
  learning_rate: 1.5e-4
  weight_decay: 0.005
  warmup_ratio: 0.08
  lr_scheduler_type: "cosine_with_restarts"
  
  # Optimization
  optim: "paged_adamw_32bit"
  max_grad_norm: 2.0
  dataloader_pin_memory: false
  
  # Evaluation and logging
  evaluation_strategy: "steps"
  eval_steps: 400
  save_strategy: "steps"
  save_steps: 800
  logging_steps: 40
  load_best_model_at_end: true
  metric_for_best_model: "eval_vulnerability_f1"

# Data settings
data:
  max_length: 4096  # DeepSeek supports longer contexts
  train_file: "data/splits/train.jsonl"
  validation_file: "data/splits/validation.jsonl"
  test_file: "data/splits/test.jsonl"

# Multi-task learning weights
tasks:
  vulnerability_classification:
    weight: 1.0
    loss_type: "asymmetric_loss"  # Better for security applications
  severity_regression:
    weight: 0.7
    loss_type: "wing_loss"  # Better for severity prediction
  fix_generation:
    weight: 1.5  # Emphasize fix generation
    loss_type: "cross_entropy"
  explanation_generation:
    weight: 0.8
    loss_type: "cross_entropy"

# Monitoring and experiment tracking
experiment:
  project_name: "contract-ai-auditor"
  run_name: "deepseek-coder-6.7b-baseline"  
  tags: ["solidity", "security", "audit", "deepseek"]
  use_wandb: true
  
# Hardware settings
system:
  mixed_precision: "bf16"
  dataloader_num_workers: 2  # Reduce for large model
  disable_tqdm: false
  report_to: ["wandb", "tensorboard"]